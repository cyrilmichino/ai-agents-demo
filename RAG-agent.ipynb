{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678e71ce",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "In this notebook, we'll showcase how to build an AI agent that implements retrieval augmented generation powered by a Vector Database. This will allow you to feed external data sources (particularly large documents that would exceed LLM context windows) into your agent and get responses grounded on your ingested data sources (the source of truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b42f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import requests\n",
    "import langsmith\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027e54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# LLM API configuration\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# LangSmith configuration\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\", \"true\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\", \"default\")\n",
    "LANGSMITH_WORKSPACE_ID = os.getenv(\"LANGSMITH_WORKSPACE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27de8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\") # Embeddings model\n",
    "vector_store = InMemoryVectorStore(embeddings) # In-memory vector store to hold document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19dd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc2c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60dca329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7d27a70d-3cd0-445f-9a0c-52190884ce5d', '8da83da9-8e64-4595-aa20-e327c4e99513', '7e5a6f5d-09a0-4af7-aaf9-2eb1b06c0c2d', '3dd07bb6-092c-468f-9280-789ff2f5a2df', '93bc26c1-c7b7-49c2-aac3-31e76c29dbc9', 'c337bdf1-106e-4aff-aa2d-dfdfdae9e7ec', '9e24761b-dbf7-404e-a61b-5c1b556c8b74', '2b18349d-8ea8-4e89-998b-b51f02bf424e', '8ebb2d26-51b4-40f9-860b-1177f54b97be', 'ef1d8372-1684-4c3e-8f66-e9aca4e715ca', '8ac8fb32-e9ee-43f9-8f40-d91126f24d40', '1731d207-b76d-4eff-a47c-806c9666b55f', '95e443f5-9d77-4162-ae4e-d963e8639296', 'fe671e4b-32b9-4a59-a767-6a01800363f8', '8a365027-8d54-42f7-a972-86b888d992ec', '4b0474ec-c93e-4832-add3-5af881263eea', '7d487f82-8691-4bf6-8d4f-296ac0ccf668', 'd9454301-6a29-45bf-915d-09b57e456ebc', '3b35603c-c44c-4726-939d-2833a90f106f', 'a621040f-bacb-4829-8520-e730d9098ebc', 'c3d7b8d4-c449-4b69-a12d-81bad272b8ab', 'b71200e7-8dce-4250-9be3-972645fd9928', '1e8f5a5e-4af4-4e48-84ee-a31b3bb406f8', '0d5fc988-998b-47f2-8ea3-e9500ea44636', 'b02be6b5-112f-4d66-880c-4fd1f6368dc3', '9daf3903-d876-4d9f-a696-443124acf7e1', '309611b5-fffc-4e40-b534-b802a8600ca3', '8f5a8895-0266-48f4-b042-af2e90f337ca', 'c7f1925a-94b4-4b60-aa9a-22741c856a39', 'dceec092-1c95-4eed-8099-27d84e1c58b4', '231376b5-ae20-4084-8d9a-85528c692dcc', '000efbc7-2197-428b-b873-1ff781c64866', '24fbc16d-4946-4198-b82a-0ff37db066c9', '61439b67-f102-4a5b-9782-c0104dd61b2d', '7fea08bd-314c-4cd3-a247-551bc290bb52', '0f18ddf8-4d95-4c15-8570-fcbc3fe93f28', 'd4584a35-fe6d-4708-90b9-46a24b8b97cb', 'a250a394-bdec-4266-9478-2564267340d6', 'f9be1a0c-243e-4fad-b052-e407e08f3720', 'd3a9ada1-1568-4694-8673-6b80d38ee0e5', '83a150d7-e069-453b-9430-a93567d0f0a1', 'ddf20e0f-d9b1-4785-80f8-b62c3244405a', '57b0e688-cffb-448f-bc61-52954570847d', '6da3c626-4105-43d0-8524-f91e57911a28', '81596bc8-c320-45c4-ba1e-ea7d37bcfddc', '435b2484-a945-46e5-84b2-f868bea3dd9d', '62d1bc7a-edc2-407f-880c-6e48264d23a1', 'b3c65e82-8428-4a13-b30c-eaf91e45aae4', '07f0ff3d-8f11-4f69-857c-0bc56644bf95', '7370554e-1e1f-42ea-8337-2ab67b0943e6', '799a13c8-60d2-419a-ab98-2630841116f7', '93e58fb7-61f2-4b54-ad31-f0981ab7b8b4', 'e4790599-04ca-40e4-a8e6-32f85ef0403e', '36d0f698-86d1-45a5-bf76-8a82832c59d9', 'f355ed7e-a2cc-4742-8d7a-484adf0043e5', 'a620f1ba-248e-4d9a-a93b-0240a226333f', 'dd38751c-13b3-40c7-8b16-ae954a6b08ac', 'b80ae879-2240-47c5-9c2d-e9d37eaece09', '327fee32-8902-4781-8b42-dda86f8827ae', 'c770c3ab-2f89-4e27-b4b7-079fc50b0202', '508a650f-643c-4a64-8503-6625ce14135e', '5139222d-f4e9-421d-abca-31dfb24fc843', '785c7de5-9d37-4192-bee9-4459fcc522cd']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bd89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce72df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer based on the documents:\n",
      "Chain of Thought (CoT) is a standard prompting technique used to improve the performance of models on complex tasks. It involves instructing the model to \"think step by step,\" which allows it to utilize more computation during test time. The primary purpose of CoT is to decompose difficult tasks into smaller, simpler, and more manageable steps. This process not only helps in solving complex problems but also provides insight into the model's reasoning process.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_generate(query: str, k: int = 5):\n",
    "    # Retrieve relevant documents\n",
    "    results = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    \n",
    "    # Create messages with context\n",
    "    messages = [\n",
    "        SystemMessage(content=f'''You are an expert assistant. Use the provided context to answer the user's question accurately and comprehensively. \n",
    "        If the answer cannot be found in the context, say so clearly.\n",
    "        \n",
    "        Context:\n",
    "        {context}'''),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    \n",
    "    # Generate response\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "user_query = input(\"Enter your question about the documents: \")\n",
    "answer = retrieve_and_generate(user_query)\n",
    "print(\"\\nAnswer based on the documents:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
