{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fefbe97",
   "metadata": {},
   "source": [
    "# Simple AI Agent\n",
    "Below is a demo of an AI agent with no tool calls, RAG workflows, or memory. The simple agent involves just a prompt i.e. Human Message + System Message and an LLM response afterwards.\n",
    "- We'll be using the Langchain Python Library to build the agent (Install all dependencies on `requirements.txt`)\n",
    "- We'll be using Gemini Flash Models which is accessible for free on OpenRouter or Google AI Studio (encode API Key on `.env` file)\n",
    "\n",
    "**Optional but Highly Recommended:** Implement tracing across all your agents to monitor performance, tokens, and costs. Langsmith is the ideal choice if working with Langchain as tracing is auto-implemented if you have a Langsmith API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29983232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# LLM API configuration\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# LangSmith configuration (Implements tracing automatically)\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\", \"true\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\", \"default\")\n",
    "LANGSMITH_WORKSPACE_ID = os.getenv(\"LANGSMITH_WORKSPACE_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cae81a",
   "metadata": {},
   "source": [
    "## Instantiate the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Google Gemini LLM model\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd608962",
   "metadata": {},
   "source": [
    "## Encode Prompts and Get Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f189de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input: what are some libraries for building AI agents on top of LLMs in Python\n",
      "Building AI agents on top of Large Language Models (LLMs) in Python involves orchestrating LLM calls, managing memory, enabling tool use, and facilitating complex reasoning. Several excellent libraries have emerged to simplify this process, each with its strengths and focus.\n",
      "\n",
      "Here's a detailed look at some of the most prominent and useful Python libraries:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. LangChain\n",
      "\n",
      "*   **Overview:** LangChain is arguably the most comprehensive and widely adopted framework for developing LLM-powered applications. It provides a structured way to chain together different components (LLMs, prompt templates, parsers, memory, tools) to build complex agents.\n",
      "*   **Key Features for Agents:**\n",
      "    *   **Chains:** Combine LLMs with other components (e.g., `LLMChain`, `SequentialChain`, `RetrievalQAChain`).\n",
      "    *   **Agents:** Allow LLMs to decide which actions to take and in what order, often using tools. It supports various agent types (e.g., `ReAct`, `OpenAIFunctionsAgent`, `PlanAndExecuteAgent`).\n",
      "    *   **Tools:** Define functions or external APIs that an agent can call (e.g., search engines, calculators, custom APIs).\n",
      "    *   **Memory:** Store and retrieve past interactions, enabling conversational context (e.g., `ConversationBufferMemory`, `VectorStoreRetrieverMemory`).\n",
      "    *   **Retrievers:** Integrate with vector databases and document loaders for Retrieval Augmented Generation (RAG), allowing agents to access external knowledge.\n",
      "    *   **Callbacks:** Monitor and log agent execution steps.\n",
      "*   **Strengths:**\n",
      "    *   Extremely flexible and modular.\n",
      "    *   Vast number of integrations with LLMs, vector stores, document loaders, etc.\n",
      "    *   Large and active community, extensive documentation.\n",
      "    *   Supports complex agentic reasoning patterns.\n",
      "*   **Use Cases:** Building chatbots, question-answering systems, autonomous agents, data analysis assistants, code generation tools.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. LlamaIndex (formerly GPT Index)\n",
      "\n",
      "*   **Overview:** While LangChain focuses on orchestration, LlamaIndex specializes in data ingestion, indexing, and retrieval for LLM applications, particularly for Retrieval Augmented Generation (RAG). It's designed to help LLMs interact with your private or domain-specific data.\n",
      "*   **Key Features for Agents:**\n",
      "    *   **Data Connectors:** Load data from various sources (APIs, databases, PDFs, Notion, etc.).\n",
      "    *   **Data Indexing:** Create structured representations of your data (e.g., vector stores, knowledge graphs, tree indexes) for efficient retrieval.\n",
      "    *   **Query Engines:** Interface with indexes to retrieve relevant information based on a query.\n",
      "    *   **Agents:** LlamaIndex also offers its own agent framework, often used in conjunction with its powerful RAG capabilities. These agents can decide which data source or tool to query.\n",
      "    *   **Tool Calling:** Integrate with external tools, similar to LangChain.\n",
      "*   **Strengths:**\n",
      "    *   Excellent for RAG and grounding LLMs with custom data.\n",
      "    *   Optimized for data retrieval performance.\n",
      "    *   Strong focus on structured data and knowledge graphs.\n",
      "    *   Can be used independently or alongside LangChain (e.g., LlamaIndex for RAG, LangChain for agent orchestration).\n",
      "*   **Use Cases:** Building knowledge-based chatbots, enterprise search, data analysis over proprietary datasets, document summarization.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. OpenAI Python SDK (with Assistants API)\n",
      "\n",
      "*   **Overview:** OpenAI's official Python library provides direct access to their powerful models. Crucially, their **Assistants API** offers a high-level, managed solution for building agent-like experiences directly within the OpenAI ecosystem.\n",
      "*   **Key Features for Agents (Assistants API):**\n",
      "    *   **Assistants:** Persistent entities that can be configured with instructions, models, tools, and files.\n",
      "    *   **Threads:** Manage conversational state and history.\n",
      "    *   **Messages:** User and assistant messages within a thread.\n",
      "    *   **Tools:** Built-in tools like Code Interpreter and Retrieval (for RAG over uploaded files), and custom function calling.\n",
      "    *   **Runs:** Execute an Assistant on a thread, which automatically handles tool use, code execution, and response generation.\n",
      "*   **Strengths:**\n",
      "    *   Official and always up-to-date with the latest OpenAI models and features.\n",
      "    *   Assistants API simplifies agent development significantly by abstracting away much of the orchestration.\n",
      "    *   Built-in powerful tools (Code Interpreter, Retrieval) are highly effective.\n",
      "    *   Managed service reduces operational overhead.\n",
      "*   **Weaknesses:**\n",
      "    *   Vendor lock-in to OpenAI.\n",
      "    *   Less flexible for highly custom agent logic compared to LangChain.\n",
      "    *   Can be more expensive for high-volume usage due to API costs.\n",
      "*   **Use Cases:** Building sophisticated chatbots, data analysis assistants, code generation tools, and any application where you want to leverage OpenAI's advanced capabilities with minimal setup.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Hugging Face `transformers`\n",
      "\n",
      "*   **Overview:** While not an agent framework itself, the `transformers` library is fundamental if you plan to use open-source LLMs (like Llama 2, Mistral, Falcon, etc.) locally or via Hugging Face's inference endpoints as the \"brain\" of your agent. It provides easy access to pre-trained models, tokenizers, and pipelines.\n",
      "*   **Key Features for Agents:**\n",
      "    *   **Model Loading:** Load various LLMs (and other NLP models) from the Hugging Face Hub.\n",
      "    *   **Tokenization:** Prepare text for LLM input and process LLM output.\n",
      "    *   **Pipelines:** High-level API for common NLP tasks, including text generation.\n",
      "    *   **Fine-tuning:** Tools for adapting models to specific tasks or datasets.\n",
      "*   **Strengths:**\n",
      "    *   Access to a vast ecosystem of open-source models.\n",
      "    *   Flexibility to run models locally or on custom infrastructure.\n",
      "    *   Essential for research and custom model development.\n",
      "*   **Use Cases:** When you need to use a specific open-source LLM as the core of your agent, integrate with other open-source NLP tools, or fine-tune models for specialized agent tasks. Often used *in conjunction* with LangChain or LlamaIndex to provide the LLM backend.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. CrewAI\n",
      "\n",
      "*   **Overview:** CrewAI is a relatively newer framework specifically designed for building **multi-agent systems**. It focuses on defining roles, tasks, and processes for a \"crew\" of AI agents to collaborate and achieve a common goal.\n",
      "*   **Key Features for Agents:**\n",
      "    *   **Agents:** Define agents with specific roles, backstories, goals, and tools.\n",
      "    *   **Tasks:** Assign specific tasks to agents, including expected output.\n",
      "    *   **Process:** Orchestrate how agents collaborate (e.g., sequential, hierarchical, or even custom processes).\n",
      "    *   **Tools:** Agents can be equipped with tools to perform their tasks.\n",
      "*   **Strengths:**\n",
      "    *   Excellent for complex problems requiring multiple perspectives or specialized expertise.\n",
      "    *   Promotes a clear, structured way to design collaborative AI workflows.\n",
      "    *   Intuitive API for defining roles and tasks.\n",
      "*   **Use Cases:** Automated research, content creation teams, complex problem-solving, strategic planning, software development teams where different agents handle different parts of the process.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. AutoGen (Microsoft)\n",
      "\n",
      "*   **Overview:** AutoGen is a framework from Microsoft that enables the development of multi-agent conversation systems. It allows agents to converse with each other to solve tasks, often involving human participation.\n",
      "*   **Key Features for Agents:**\n",
      "    *   **Configurable Agents:** Define agents with different capabilities (LLM-backed, human proxy, code executor).\n",
      "    *   **Conversational Programming:** Agents interact by sending messages to each other, simulating human-like collaboration.\n",
      "    *   **Tool Use:** Agents can execute code (Python, shell) and use custom tools.\n",
      "    *   **Human-in-the-Loop:** Seamless integration of human feedback and intervention.\n",
      "*   **Strengths:**\n",
      "    *   Powerful for multi-agent collaboration and complex task automation.\n",
      "    *   Strong emphasis on code execution and human interaction.\n",
      "    *   Flexible agent roles and communication patterns.\n",
      "*   **Use Cases:** Automated software development, complex data analysis, research assistants, interactive problem-solving where agents can write and execute code.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Guidance (Microsoft)\n",
      "\n",
      "*   **Overview:** Guidance is a library for controlling LLMs with a syntax that blends generation, prompting, and logical control into a single, powerful template language. It's not an agent framework itself but is incredibly useful *within* an agent framework for precise control over LLM output.\n",
      "*   **Key Features:**\n",
      "    *   **Structured Prompting:** Define prompts with variables, conditional logic, and loops.\n",
      "    *   **Constrained Generation:** Force LLMs to generate output in specific formats (e.g., JSON, specific keywords, regular expressions).\n",
      "    *   **Tool Use Integration:** Can be used to define how an LLM calls tools and parses their output.\n",
      "*   **Strengths:**\n",
      "    *   Provides fine-grained control over LLM generation.\n",
      "    *   Ensures reliable and structured output, crucial for agent decision-making.\n",
      "    *   More intuitive than raw prompt engineering for complex output formats.\n",
      "*   **Use Cases:** Ensuring agents generate valid JSON for tool calls, extracting structured information from text, generating code snippets with specific syntax, controlling the flow of an LLM's thought process.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Marvin (Prefect)\n",
      "\n",
      "*   **Overview:** Marvin is a lightweight, Pythonic library that makes it easy to use LLMs for structured data extraction, classification, and function calling. It leverages Pydantic for defining expected output schemas.\n",
      "*   **Key Features:**\n",
      "    *   **Structured Output:** Define Pydantic models to guide LLMs to generate structured data.\n",
      "    *   **Type-Hinted Functions:** Turn regular Python functions into LLM-powered functions (e.g., for classification, summarization).\n",
      "    *   **AI Models:** Create \"AI models\" that can perform tasks like entity extraction or data transformation.\n",
      "    *   **Tool Calling:** Simplifies the process of defining and using tools with LLMs.\n",
      "*   **Strengths:**\n",
      "    *   Very Pythonic and easy to integrate into existing Python codebases.\n",
      "    *   Excellent for ensuring structured and reliable LLM output.\n",
      "    *   Lightweight compared to full frameworks like LangChain.\n",
      "*   **Use Cases:** Data extraction from unstructured text, content moderation, sentiment analysis, converting natural language commands into structured function calls for agents.\n",
      "\n",
      "---\n",
      "\n",
      "### Which one to choose?\n",
      "\n",
      "*   **For general-purpose agent building and maximum flexibility:** **LangChain** is your go-to.\n",
      "*   **For heavy data integration and RAG:** **LlamaIndex** is essential, often used alongside LangChain.\n",
      "*   **For a managed, high-level agent solution within the OpenAI ecosystem:** The **OpenAI Assistants API** is a strong contender.\n",
      "*   **For multi-agent collaboration and complex workflows:** **CrewAI** or **AutoGen** are excellent choices.\n",
      "*   **For fine-grained control over LLM output and structured generation:** **Guidance** or **Marvin** can be integrated into any of the above frameworks.\n",
      "*   **For using open-source LLMs locally:** **Hugging Face `transformers`** is fundamental.\n",
      "\n",
      "Many advanced agent systems will combine several of these libraries, leveraging the strengths of each to create robust and intelligent applications.\n"
     ]
    }
   ],
   "source": [
    "## User Input â€“ Get a question from the user\n",
    "user_input = input(\"Enter your question: \")\n",
    "\n",
    "## Encode Prompts and Get Response\n",
    "messages = [\n",
    "    SystemMessage(content=f'''You are an expert in this topic. Answer the users professionally and with details.\n",
    "        '''),\n",
    "    HumanMessage(content=f\"{user_input}\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"user_input: {user_input}\")\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
